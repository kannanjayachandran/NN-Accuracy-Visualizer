{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0       0       0       0       0       0       0       0       0       0   \n",
       "1       0       0       0       0       0       0       0       0       0   \n",
       "2       0       0       0       0       0       0       0       0       0   \n",
       "3       0       0       0       0       0       0       0       0       0   \n",
       "4       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel9  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 784 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./Data/train.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the input into a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of data :\n",
      " rows: 42000 and columns: 785\n"
     ]
    }
   ],
   "source": [
    "data = np.array(data)\n",
    "m, n = data.shape\n",
    "print(f'shape of data :\\n rows: {m} and columns: {n}')\n",
    "\n",
    "# randomly shuffling data before splitting into testing and training sets\n",
    "np.random.shuffle(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> There are 42000 rows and 785 columns; that means we have 42000 examples row wise (each row is an image of total 784 pixel (28*28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing testing data\n",
    "\n",
    "- Test data contains first 1000 rows\n",
    "- `data_test` variable before transposing would contain 1000 example row wise; after transposing, it would contain 1000 examples column wise. \n",
    "- `y_test` variable contains the labels (The thing we want to predict (1000 numbers between 0 and 9)\n",
    "- `x_test` variable contains the features (The things we use to predict labels (1000 - 784 long numpy arrays with pixel value[0-255]))\n",
    "- We need to normalize the `x_test` variable's pixel values which are usually in the range of 0 to 255, to a range of 0 to 1 to prevent certain features from dominating the model due to their scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data_test (After transposing): (785, 1000)\n",
      "\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "data_test = data[0:1000].T \n",
    "print(f'Shape of data_test (After transposing): {data_test.shape}\\n')\n",
    "\n",
    "y_test = data_test[0]\n",
    "x_test = data_test[1:n]\n",
    "x_test = x_test / 255.\n",
    "\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data[1000:m].T\n",
    "y_train = data_train[0]\n",
    "x_train = data_train[1:n]\n",
    "x_train = x_train / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41000\n"
     ]
    }
   ],
   "source": [
    "_, m_train = x_train.shape\n",
    "\n",
    "print(m_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 1, 0, ..., 0, 0, 6], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to initialize weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_parameters():\n",
    "    W1 = np.random.rand(10, 784) - 0.5\n",
    "    B1 = np.random.rand(10, 1) - 0.5\n",
    "    W2 = np.random.rand(10, 10) - 0.5\n",
    "    B2 = np.random.rand(10, 1) - 0.5\n",
    "    return W1, W2, B1, B2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLu Activation function\n",
    "\n",
    "- ReLu is a non-linear activation function which is used to introduce non-linearity in the model. It would return 0 if the input is negative and the input itself if the input is positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(arr):\n",
    "    return np.maximum(arr, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU derivative\n",
    "\n",
    "- This is the derivative of the ReLu function. It would return 0 if the input is negative and 1 if the input is positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derv_ReLU(arr):\n",
    "    return arr > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SoftMax Activation function\n",
    "\n",
    "- SoftMax is a non-linear activation function which is used to squash the output of the model to a range of 0 to 1 (a probability distribution). It is used in the output layer of the model. It is also used to calculate the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softMax(arr):\n",
    "    sm = np.exp(arr) / sum(np.exp(arr))\n",
    "    return sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding\n",
    "\n",
    "- One hot encoding is a process of converting categorical data into a form that could be provided to ML algorithms to do a better job in prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(y):\n",
    "    one_hot_y = np.zeros((y.size, y.max() + 1))\n",
    "    one_hot_y[np.arange(y.size), y] = 1\n",
    "    one_hot_y = one_hot_y.T\n",
    "    return one_hot_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation\n",
    "\n",
    "- Forward propagation is the process of calculating the output of the model using the input and the weights and biases of the model. It is also called the inference step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(W1, W2, B1, B2, x):\n",
    "    Z1 = W1.dot(x) + B1\n",
    "    A1 = ReLU(Z1)\n",
    "    Z2 = W2.dot(A1) + B2\n",
    "    A2 = softMax(Z2)\n",
    "    return Z1, Z2, A1, A2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Propagation\n",
    "\n",
    "- Backward propagation is the process of calculating the gradients of the loss function with respect to the weights and biases. The gradients are used to update the weights and biases in the direction of the minimum of the loss function.\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial z} \\frac{\\partial z}{\\partial w}$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial z} \\frac{\\partial z}{\\partial b}$$\n",
    "\n",
    "---\n",
    "\n",
    "$$dZ^{[2]} = A^{[2]} - Y$$\n",
    "$$dW^{[2]} = \\frac{1}{m} dZ^{[2]} A^{[1]T}$$\n",
    "$$dB^{[2]} = \\frac{1}{m} \\Sigma {dZ^{[2]}}$$\n",
    "$$dZ^{[1]} = W^{[2]T} dZ^{[2]} .* g^{[1]\\prime} (z^{[1]})$$\n",
    "$$dW^{[1]} = \\frac{1}{m} dZ^{[1]} A^{[0]T}$$\n",
    "$$dB^{[1]} = \\frac{1}{m} \\Sigma {dZ^{[1]}}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation(Z1, Z2, A1, A2, W1, W2, x, y):\n",
    "    one_hot_y = one_hot(y)\n",
    "\n",
    "    dZ2 = A2 - one_hot_y\n",
    "    dW2 = (1 / m) * dZ2.dot(A1.T)\n",
    "    db2 = (1/m) * np.sum(dZ2)\n",
    "\n",
    "    dZ1 = W2.T.dot(dZ2) * derv_ReLU(Z1)\n",
    "    dW1 = (1/m) * dZ1.dot(x.T)\n",
    "    db1 = (1/m) * np.sum(dZ1)\n",
    "    \n",
    "    return dW1, dW2, db1, db2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to update weights and biases\n",
    "\n",
    "- The weights and biases are updated in the direction of the minimum of the loss function using the gradients calculated in the backward propagation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(W1, W2, B1, B2, dW1, dW2, db1, db2, alpha):\n",
    "    W1 = W1 - alpha * dW1\n",
    "    B1 = B1 - alpha * db1\n",
    "    W2 = W2 - alpha *dW2\n",
    "    B2 =  B2 - alpha * db2\n",
    "\n",
    "    return W1, W2, B1, B2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to predict\n",
    "\n",
    "- This function is used to predict the labels of the test data. The `argmax` function of numpy is used to get the index of the maximum value in the output of the model. The index of the maximum value is the predicted label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(A2):\n",
    "    return np.argmax(A2, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to calculate accuracy\n",
    "\n",
    "- This function is used to calculate the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(predictions, y):\n",
    "    return np.sum(predictions == y) / y.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent Algorithm\n",
    "\n",
    "- The gradient descent algorithm is used to update the weights and biases of the model in the direction of the minimum of the loss function. The learning rate is used to control the step size of the gradient descent algorithm.\n",
    "\n",
    "$$w = w - \\alpha \\frac{\\partial L}{\\partial w}$$\n",
    "\n",
    "$$b = b - \\alpha \\frac{\\partial L}{\\partial b}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, alpha, iteration):\n",
    "\n",
    "    # Initializing the weight and biases\n",
    "    w1, w2, b1, b2 = init_parameters()\n",
    "\n",
    "    for i in range(iteration):\n",
    "        z1, z2, a1, a2 = forward_propagation(w1, w2, b1, b2, x)\n",
    "\n",
    "        d_w1, d_w2, db1, db2 = back_propagation(z1, z2, a1, a2, w1, w2, x, y)\n",
    "\n",
    "        w1, w2, b1, b2 = update_parameters(w1, w2, b1, b2, d_w1, d_w2, db1, db2, alpha)\n",
    "\n",
    "        # logging\n",
    "        if i % 10 == 0:\n",
    "            print(f'Iteration: {i}\\n')\n",
    "            predictions = get_predictions(a2)\n",
    "            print(get_accuracy(predictions, y), '\\n') \n",
    "    return w1, w2, b1, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "\n",
      "0.07234146341463414 \n",
      "\n",
      "Iteration: 10\n",
      "\n",
      "0.1245609756097561 \n",
      "\n",
      "Iteration: 20\n",
      "\n",
      "0.2050731707317073 \n",
      "\n",
      "Iteration: 30\n",
      "\n",
      "0.2786585365853659 \n",
      "\n",
      "Iteration: 40\n",
      "\n",
      "0.3474634146341463 \n",
      "\n",
      "Iteration: 50\n",
      "\n",
      "0.4009268292682927 \n",
      "\n",
      "Iteration: 60\n",
      "\n",
      "0.4501707317073171 \n",
      "\n",
      "Iteration: 70\n",
      "\n",
      "0.4918292682926829 \n",
      "\n",
      "Iteration: 80\n",
      "\n",
      "0.5255365853658537 \n",
      "\n",
      "Iteration: 90\n",
      "\n",
      "0.5566585365853659 \n",
      "\n",
      "Iteration: 100\n",
      "\n",
      "0.5829756097560975 \n",
      "\n",
      "Iteration: 110\n",
      "\n",
      "0.6065853658536585 \n",
      "\n",
      "Iteration: 120\n",
      "\n",
      "0.628609756097561 \n",
      "\n",
      "Iteration: 130\n",
      "\n",
      "0.6479024390243903 \n",
      "\n",
      "Iteration: 140\n",
      "\n",
      "0.6652439024390244 \n",
      "\n",
      "Iteration: 150\n",
      "\n",
      "0.6811951219512196 \n",
      "\n",
      "Iteration: 160\n",
      "\n",
      "0.6958536585365853 \n",
      "\n",
      "Iteration: 170\n",
      "\n",
      "0.7089512195121951 \n",
      "\n",
      "Iteration: 180\n",
      "\n",
      "0.7198536585365853 \n",
      "\n",
      "Iteration: 190\n",
      "\n",
      "0.7300487804878049 \n",
      "\n",
      "Iteration: 200\n",
      "\n",
      "0.7394634146341463 \n",
      "\n",
      "Iteration: 210\n",
      "\n",
      "0.7479268292682927 \n",
      "\n",
      "Iteration: 220\n",
      "\n",
      "0.7556829268292683 \n",
      "\n",
      "Iteration: 230\n",
      "\n",
      "0.7621951219512195 \n",
      "\n",
      "Iteration: 240\n",
      "\n",
      "0.7689268292682927 \n",
      "\n",
      "Iteration: 250\n",
      "\n",
      "0.7739512195121951 \n",
      "\n",
      "Iteration: 260\n",
      "\n",
      "0.779390243902439 \n",
      "\n",
      "Iteration: 270\n",
      "\n",
      "0.7839756097560976 \n",
      "\n",
      "Iteration: 280\n",
      "\n",
      "0.7884146341463415 \n",
      "\n",
      "Iteration: 290\n",
      "\n",
      "0.7925365853658537 \n",
      "\n",
      "Iteration: 300\n",
      "\n",
      "0.796219512195122 \n",
      "\n",
      "Iteration: 310\n",
      "\n",
      "0.7992439024390244 \n",
      "\n",
      "Iteration: 320\n",
      "\n",
      "0.8017804878048781 \n",
      "\n",
      "Iteration: 330\n",
      "\n",
      "0.8047073170731708 \n",
      "\n",
      "Iteration: 340\n",
      "\n",
      "0.8079024390243903 \n",
      "\n",
      "Iteration: 350\n",
      "\n",
      "0.8104878048780488 \n",
      "\n",
      "Iteration: 360\n",
      "\n",
      "0.8127073170731707 \n",
      "\n",
      "Iteration: 370\n",
      "\n",
      "0.8145121951219512 \n",
      "\n",
      "Iteration: 380\n",
      "\n",
      "0.8166097560975609 \n",
      "\n",
      "Iteration: 390\n",
      "\n",
      "0.8184390243902439 \n",
      "\n",
      "Iteration: 400\n",
      "\n",
      "0.8204390243902439 \n",
      "\n",
      "Iteration: 410\n",
      "\n",
      "0.8226097560975609 \n",
      "\n",
      "Iteration: 420\n",
      "\n",
      "0.8247560975609756 \n",
      "\n",
      "Iteration: 430\n",
      "\n",
      "0.8267317073170731 \n",
      "\n",
      "Iteration: 440\n",
      "\n",
      "0.8285121951219512 \n",
      "\n",
      "Iteration: 450\n",
      "\n",
      "0.8302439024390244 \n",
      "\n",
      "Iteration: 460\n",
      "\n",
      "0.8320487804878048 \n",
      "\n",
      "Iteration: 470\n",
      "\n",
      "0.8337073170731707 \n",
      "\n",
      "Iteration: 480\n",
      "\n",
      "0.8351219512195122 \n",
      "\n",
      "Iteration: 490\n",
      "\n",
      "0.836780487804878 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "W1, W2, B1, B2 = gradient_descent(x_train, y_train, 0.10, 500)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model on the test data\n",
    "\n",
    "- The model is tested on the test data and the accuracy is calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pred(x, w1, b1, w2, b2):\n",
    "    _, _, _, a2 = forward_propagation(w1, w2, b1, b2, x)\n",
    "    predictions = get_predictions(a2)\n",
    "    return predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.845"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_p = make_pred(x_test, W1, B1, W2, B2)\n",
    "get_accuracy(test_p, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Saving the model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open ('NN.pickle', 'wb') as f:\n",
    "    pickle.dump([W1, W2, B1, B2], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def test_predictions(index, w1, b1, w2, b2):\n",
    "    current_image = x_train[:, index, None]\n",
    "    predictions = make_pred(x_train[:, index, None], w1, b1, w2, b2)\n",
    "    label = y_train[index]\n",
    "\n",
    "    print('prediction: ', predictions)\n",
    "    print(\"label: \", label)\n",
    "\n",
    "    current_image = current_image.reshape((28, 28)) * 255\n",
    "\n",
    "    plt.gray()\n",
    "    plt.imshow(current_image, interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction:  [5]\n",
      "label:  5\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_predictions(504, W1, B1, W2, B2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN-strlit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 | packaged by conda-forge | (main, Jan 11 2023, 15:15:40) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "21c3c7cb9fc2468ce80fc2f5365b4498bfef0bae8a8af78ee1d6c2c285602b31"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
